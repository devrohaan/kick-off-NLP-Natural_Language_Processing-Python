[![Wisdomic Panda](https://github.com/robagwe/wisdomic-panda/blob/master/imgs/panda.png)](http://www.rohanbagwe.com/)  **Wisdomic Panda**
> *Hold the Vision, Trust the Process.*


## Natural Language Processing: A Py Kick-off Digest! 
###### Natural Language Processing in Python.

# Data Extraction
*... datasets to practice on when getting started with natural language processing.*

![data](https://github.com/robagwe/wisdomic-panda/blob/master/imgs/data.gif)


One of the first things required for natural language processing tasks is a corpus. In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.). Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.


The good thing is that the internet is filled with text, and in many cases this text is collected and well oganized, even if it requires some finessing into a more usable, precisely-defined format. Wikipedia, in particular, is a rich source of well-organized textual data. 

Every day, we generate huge amounts of text online, creating vast quantities of data about what is happening in the world and what people think. All of this text data is an invaluable resource that can be mined in order to generate meaningful business insights for analysts and organizations. 

It's also a vast collection of knowledge, and the unhampered mind can dream up all sorts of uses for just such a body of text.
What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.

> :pushpin: Human civilization is drowning in data. According to the Netcraft January 2018 Web Server, Survey there are 1,805,260,010 (over 1.8 billion) websites. Yes, the Internet is very big. However, most of those websites get almost no visitors.
In 2008, Google reported that the web had one trillion pages. IN 2013, it estimated the web at 30 trillion pages. Merrill Lynch projects that available data will expand to 40 zettabytes by 2020. These estimates include video and image data, as well as structured data in databases. However, most of it is plain old text. 

> :pushpin: Data is being called the new oil. It’s changing the meaning of analytics and advancing the Artificial Intelligence revolution every day. Data is fueling the future as we speak and getting onboard a long sailing ship is a good idea.



With the ongoing growth of the World Wide Web and social media, there is a drastic increase in online data. As the amount of data increases the mechanisms to process these unstructured data and to extract meaningful information from it becomes more challenging.
Today, natural language processing enthusiasts have many options when it comes to acquiring annotated datasets to train their algorithms. 


NLP enthusiasts developing NLP models have several public datasets available to them as they begin their careers, as well as tools and solutions that make generating custom annotations far more efficient than in years past.
Ten minutes of googling will yield you a myraid of links to public datasets.

But,

*One of the reasons why it’s so hard to learn, practice and experiment with Natural Language Processing is due to the lack of structured corpora. Analyzing all of this content isn’t easy, since converting text produced by people into structured information to analyze with a machine is a complex task. Building a gold standard corpus is seriously a hard work. That’s why resources are so scarce or cost a lot of money.*

###### In this diegst, I’m going to aggregate some cool resources, some very well known, some a bit under the radar. If you need specialized, custom training data (say, sample conversations with some unique specifics for training a chatbot), you’re gonna need to create that yourself (or have it created). 

To extract data from Internet, I rely heavily on two major techniques given below.

### 1. [Web Scraping](https://github.com/robagwe/kick-off-web-scraping-python-selenium-beautifulsoup)
Kindly visit my [kick-off-web-scraping-python-selenium-beautifulsoup](https://github.com/robagwe/kick-off-web-scraping-python-selenium-beautifulsoup) for quick understanding of web scraping.
### 2. Online Datasets

   2.1 [Wikipedia](https://github.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/wiki_Corpus/wikipedia_DataExtract.py)
   
   2.2 [Twitter](https://github.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/twitter/twitterDataExtract.py)

### 3. Examples:

   3.1 [Glassdoor_jobs](https://github.com/robagwe/kick-off-web-scraping-python-selenium-beautifulsoup/tree/master/glassdoor_jobs)
   
   3.2 [Pablo_quotes](https://github.com/robagwe/kick-off-web-scraping-python-selenium-beautifulsoup/tree/master/pablo_quotes)
   
   3.3 [Premier_League_score_table](https://github.com/robagwe/kick-off-web-scraping-python-selenium-beautifulsoup/tree/master/premier_league_score_table)
   
   3.4 [Bhagavad Gita Lessons](https://githu### 2. Online Datasets
 -   2.1 [Wikipedia](https://github.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/wiki_Corpus/wikipedia_DataExtract.py)
 -   2.2 [Twitter](https://github.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/twitter/twitterDataExtract.py)
b.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/ScrapDataK/scrapDataKCDIssue.py)
   
   3.5 [Akbar_Birbal_Stories](https://github.com/robagwe/kick-off-NLP-Natural_Language_Processing-Python/blob/master/0_Data_Extraction/Eg_Data_Extraction_WebScarping/quickDataScrap.py)

## NLP Free Datasets links & credits
*I have tried to provide a mixture of datasets that are popular for use in academic papers that are modest in size.
Almost all datasets are freely available for download today.*

:open_file_folder: [NLP for Hackers](https://nlpforhackers.io/corpora/)

:open_file_folder: [Conversational dataset for chatbots](http://freeconnection.blogspot.com/2016/04/conversational-datasets-for-train.html)

:open_file_folder: [Jason Brownlee ](https://machinelearningmastery.com/datasets-natural-language-processing/)

:open_file_folder: [Jason Brownlee ](https://machinelearningmastery.com/datasets-natural-language-processing/)

:open_file_folder: [Pre-Existing, Publicly Available Datasets](https://mighty.ai/blog/training-data-for-nlp-algorithms-your-options-for-collecting-or-creating-annotated-datasets/)

:open_file_folder: [Ultimate, Personal Favourite](https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/)

:pushpin: **NOTE:**

It is better to use small datasets that you can download quickly and do not take too long to fit models. Further, it is also helpful to use standard datasets that are well understood and widely used so that you can compare your results to see if you are making progress.


> If your favorite dataset is not listed or you think you know of a better dataset that should be listed, please feel free to contact me at :email: robagwe@gmail.com or open a pull request to contribute.
Seriously, It would be great to discuss Technology.





